{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this tutorial we will cover how to use autodp for computing the most commonly used DP learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noisy Full Gradient Descent\n",
    "\n",
    "Suppose we are minimizing the following objective function:\n",
    "$$\\min_\\theta \\sum_{i=1}^n \\ell(z_i,\\theta)$$\n",
    "Let parameter be $\\theta\\in \\mathbb{R}^d$ and data points be $z_1,...,z_n \\in \\mathcal{Z}$.\n",
    "\n",
    "The noisy gradient descent algorithm iteratively updates\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta_t \\left(\\sum_{i=1}^n\\nabla \\ell(z_i,\\theta_t) + \\mathcal{N}(0,\\Delta_t^2\\sigma_t^2  I_d)\\right). $$\n",
    "\n",
    "In the above, $\\sigma_t$ are determined ahread of time, $\\Delta_t$ is an upper bound of $\\sup_{z\\in\\mathcal{Z}} \\|\\nabla\\ell(z,\\theta_t)\\|_2$, i.e., the **global sensitivity** of the full gradient $\\sum_{i=1}^n\\nabla \\ell(z_i,\\theta_t)$.\n",
    "\n",
    "This algorithm from the differential privacy perspective, can be viewed as a sequence of standard Gaussian mechanisms (which admits tight composition and tight calibration) and you can use the following code-snippet to tightly compute the privacy loss after you run this algorithm.\n",
    "\n",
    "### Cautionary notes:\n",
    "\n",
    "- The objective function is a summation, not an empirical average. If it is an average you need to appropriately scale your update rule by a factor of $1/n$ **outside the brackets**.\n",
    "- When $\\Delta_t$ is unknown, you can ensure DP by clipping the *per-instance* gradient, i.e., use the sum of \n",
    "$$\n",
    "\\min\\{1,\\frac{\\Delta_t}{\\|\\nabla \\ell(z_i,\\theta_t)\\|_2 }\\}\\nabla \\ell(z_i,\\theta_t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/scipy/optimize/_optimize.py:2884: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0677358464697515"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autodp.mechanism_zoo import GaussianMechanism \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NoisyGD_mech(GaussianMechanism):\n",
    "    def __init__(self,sigma_list,name='NoisyGD'):\n",
    "        GaussianMechanism.__init__(self, sigma=np.sqrt(1/np.sum(1/sigma_list**2)),name=name)\n",
    "        self.params = {'sigma_list':sigma_list}\n",
    "\n",
    "# The user could log sigma_list and then just declare a NoisyGD_mech object.\n",
    "sigma_list = np.array([5.0,3.0,4.0])\n",
    "\n",
    "mech = NoisyGD_mech(sigma_list)\n",
    "\n",
    "# compute epsilon, as a function of delta\n",
    "mech.get_approxDP(delta=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy Stochastic Gradient Descent\n",
    " \n",
    "Now, suppose we run the popular NoisySGD with Poisson sampling. \n",
    "    \n",
    "The NoisySGD algorithm works by\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta_t \\left(\\sum_{i\\in \\mathcal{I}_t}\\min\\{1,\\frac{\\Delta_t}{\\|\\nabla \\ell(z_i,\\theta_t)\\|_2 }\\}\\nabla \\ell(z_i,\\theta_t) + \\mathcal{N}(0,\\Delta^2\\sigma^2  I_d)\\right) $$\n",
    "where the minibatch $\\mathcal{I}_t \\subset [n]$ is constructed by **flipping one independent coin for each data point $i\\in[n]$** to decide whether to include or exclude that data.\n",
    "\n",
    "\n",
    "In the above, $\\sigma_t$ are determined ahread of time, $\\Delta_t$ is the per-instance clipping factor.  If the global sensitivity $\\sup_{z} \\ell(z,\\theta_t) \\leq \\Delta_t$ (this is the Lipschitz constant. You may prove this in theor.) then clipping is not needed.\n",
    "\n",
    "This algorithm from the differential privacy perspective, can be viewed as a sequence of **subsampled Gaussian mechanisms**  and you can use the following code-snippet to compute the privacy loss after you run this algorithm based on RDP-based Analytical Moments Accountant.\n",
    "\n",
    "\n",
    "### Cautionary notes:\n",
    "\n",
    "- The objective function is a summation, not an empirical average. If it is an average you need to appropriately scale your update rule by a factor of $1/(n\\gamma)$ **outside the brackets**. Note that you cannot divide it by $|\\mathcal{I}_t|$ because it is considered privacy information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27105623043762284"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autodp.autodp_core import Mechanism\n",
    "from autodp.transformer_zoo import Composition \n",
    "from autodp import mechanism_zoo, transformer_zoo\n",
    "\n",
    "class NoisySGD_mech(Mechanism):\n",
    "    def __init__(self,prob,sigma,niter,name='NoisySGD'):\n",
    "        Mechanism.__init__(self)\n",
    "        self.name=name\n",
    "        self.params={'prob':prob,'sigma':sigma,'niter':niter}\n",
    "        \n",
    "        # create such a mechanism as in previously\n",
    "        subsample = transformer_zoo.AmplificationBySampling() # by default this is using poisson sampling\n",
    "        mech = mechanism_zoo.GaussianMechanism(sigma=sigma)\n",
    "        prob = prob\n",
    "        # Create subsampled Gaussian mechanism\n",
    "        SubsampledGaussian_mech = subsample(mech,prob,improved_bound_flag=True)\n",
    "\n",
    "        # Now run this for niter iterations\n",
    "        compose = transformer_zoo.Composition()\n",
    "        mech = compose([SubsampledGaussian_mech],[niter])\n",
    "\n",
    "        # Now we get it and let's extract the RDP function and assign it to the current mech being constructed\n",
    "        rdp_total = mech.RenyiDP\n",
    "        self.propagate_updates(rdp_total,type_of_update='RDP')\n",
    "\n",
    "        \n",
    "gamma = 0.01\n",
    "        \n",
    "noisysgd = NoisySGD_mech(prob=gamma,sigma=5.0,niter=1000)\n",
    "\n",
    "\n",
    "# compute epsilon, as a function of delta\n",
    "noisysgd.get_approxDP(delta=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Private Aggregation of Teacher Ensembles (PATE)  \n",
    "\n",
    "This is a different, and increasingly popular type of private Deep Learning algorithm in the Knowledge Transfer model.\n",
    "\n",
    "Specifically, there is a private dataset, and also a public (unlabeled) dataset. We will use the private dataset to privately release labels for the public dataset.  Let us say that there are $C$ classes.\n",
    "\n",
    "PATE splits the private dataset into $k$ disjoint parts and for each part, train a supervised learner (\"Teacher\"). Then for each data point $x_i$ from $i=1,...,m$ in the public dataset, privately release the following\n",
    "\n",
    "$$\n",
    "\\hat{p}_i =  \\frac{1}{k} \\left(\\sum_{j}  \\hat{p}^{(j)}(x_i)  + \\mathcal{N}(0, 2 \\sigma^2 I_C)\\right) \n",
    "$$\n",
    "where $\\hat{p}^{(j)}$ is the soft-max prediction function of Teacher $j$, which returns a probability distribution on the labels (a $C$-dimensional vector).\n",
    "\n",
    "\n",
    "From the differential privacy point of view this is nothing but composing $m$ Gaussian mechanisms, i.e., same as NoisyGD above. The global L2 sensitivity is $\\sqrt{2}$ (hence the factor of $2$ in front of $\\sigma^2$.) This is because adding or removing one individual data point will affect the prediction of only one of the teachers. The largest possible changes in L2 is $\\sqrt{2}$.\n",
    "\n",
    "### Notes:\n",
    "- In the above, we described the version that aggregates the soft-label and release the average soft-label from the k-teachers.  You could also aggregate the hard label,  release either the voting scores or just the  voted label. The same code-snippet below works without changes. There will be some (data-dependent) privacy benefits in doing those which we do not cover in this tutorial.\n",
    "- You could **screen the data points** and collect labels only for those that are **worthy**, so as to reduce $m$, hence the total privacy loss.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.99715121422065"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will be reusing the `NoisyGD_mech' class defined earlier. \n",
    "\n",
    "sigma = 5.0\n",
    "m=100\n",
    "\n",
    "sigma_list = sigma * np.ones(shape=(m,1))\n",
    "pate = NoisyGD_mech(sigma_list,name='PATE_gaussian')\n",
    "# compute epsilon, as a function of delta\n",
    "pate.get_approxDP(delta=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Private KNN\n",
    "\n",
    "Private KNN is an alternative algorithm in the knowledge transfer setting that allows us to label more public data points than PATE.  The idea is to subsample the data set, find the $k$ closest data points to the input $x_i$ from the sampled subset, then privately release the voted labels.\n",
    "\n",
    "1. Sample minibatch $\\mathcal{I}_t \\subset [n]$  by **flipping one independent coin for each data point $i\\in[n]$** to decide whether to include or exclude that data point.\n",
    "2. Find the closest $k$ data points within  $\\mathcal{I}_t(x_i)\\subset \\mathcal{I}_t$ using your favorite distance function $\\text{dist}(\\cdot,\\cdot)$.\n",
    "3. Private release the voting scores\n",
    "$$\n",
    "\\hat{p}_i =  \\frac{1}{k} \\left(\\sum_{j \\in \\mathcal{I}_t(x_i)}  \\textrm{OneHot}(y_j)  + \\mathcal{N}(0, 2 \\sigma^2 I_C)\\right)\n",
    "$$ \n",
    "\n",
    "\n",
    "From the DP perspective this is a composition of the Poisson Subsampled Gaussian Mechanism, which is equivalent to that of NoisySGD. So the following snippets suffices. The L2 global sensitivity is again $\\sqrt{2}$ here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27105623043762284"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will reuse the noisySGD class\n",
    "\n",
    "# number of data points\n",
    "\n",
    "gamma = 0.01\n",
    "m=1000\n",
    "sigma = 5.0\n",
    "\n",
    "privateKNN = NoisySGD_mech(prob=gamma,sigma=sigma,niter=m)\n",
    "\n",
    "# compute epsilon, as a function of delta\n",
    "privateKNN.get_approxDP(delta=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Laplace-noise version of NoisyGD, NoisySGD and PATE/PrivatekNN\n",
    "\n",
    "If you need pure differential privacy, you could replace Gaussian noise with Laplace noise above and when getting approximate DP, choose delta = 0.  \n",
    "\n",
    "You could also get a similar (or slightly smaller) epsilon when you choose delta > 0, but the noise is more heavy-tailed thus it may affect the performance of your algorithm.\n",
    "\n",
    "\n",
    "Code snippets below.\n",
    "\n",
    "### Caution:\n",
    "- Note that Laplace mechanism requires bounded L1 sensitivity, therefore clipping of the per-instance gradients need to be by clipping them to have bounded L1 norm.\n",
    "- Similarly, the global L1 sensitivity in voting schemes in PATE and PrivateKNN is 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.850346958780321\n",
      "0.2569326333160541\n"
     ]
    }
   ],
   "source": [
    "# NoisyGD and PATE with Laplace Mechanism\n",
    "\n",
    "# ------------------------------------------\n",
    "class Composed_Laplace_mech(Mechanism):\n",
    "    def __init__(self,b,niter,name='Composed_Laplace'):\n",
    "        Mechanism.__init__(self)\n",
    "        self.name=name\n",
    "        self.params={ 'b':b,'niter':niter}\n",
    "        \n",
    "        # create such a mechanism as in previously \n",
    "        laplace_mech = mechanism_zoo.LaplaceMechanism(b=b)  \n",
    "        # Now run this for niter iterations\n",
    "        compose = transformer_zoo.Composition()\n",
    "        mech = compose([laplace_mech],[niter])\n",
    "        # Now we get it and let's extract the RDP function and assign it to the current mech being constructed\n",
    "        rdp_total = mech.RenyiDP\n",
    "        self.propagate_updates(rdp_total,type_of_update='RDP')\n",
    "\n",
    "        \n",
    "#  Parameters\n",
    "b = 5.0\n",
    "m = 100\n",
    "\n",
    "NoisyGD_laplace = Composed_Laplace_mech(b=b,niter=m,name=\"NoisyGD_Laplace\")\n",
    "\n",
    "PATE_laplace = Composed_Laplace_mech(b=b,niter=m,name=\"PATE_Laplace\")\n",
    "        \n",
    "print(PATE_laplace.get_approxDP(delta=1e-6))\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# NoisySGD and PrivateKNN with Poisson Sampled Laplace Mechanism\n",
    "class Composed_SubsampledLaplace_mech(Mechanism):\n",
    "    def __init__(self,prob,b,niter,name='Composed_SubsampledLaplace'):\n",
    "        Mechanism.__init__(self)\n",
    "        self.name=name\n",
    "        self.params={'prob':prob,'b':b,'niter':niter}\n",
    "        \n",
    "        # create such a mechanism as in previously\n",
    "        subsample = transformer_zoo.AmplificationBySampling() # by default this is using poisson sampling\n",
    "        laplace_mech = mechanism_zoo.LaplaceMechanism(b=b)  \n",
    "        prob = prob\n",
    "        # Create subsampled Gaussian mechanism\n",
    "        Subsampled_laplace_mech = subsample(laplace_mech,prob,improved_bound_flag=True)\n",
    "\n",
    "        # Now run this for niter iterations\n",
    "        compose = transformer_zoo.Composition()\n",
    "        mech = compose([Subsampled_laplace_mech],[niter])\n",
    "\n",
    "        # Now we get it and let's extract the RDP function and assign it to the current mech being constructed\n",
    "        rdp_total = mech.RenyiDP\n",
    "        self.propagate_updates(rdp_total,type_of_update='RDP')\n",
    "        \n",
    "\n",
    "#  Parameters\n",
    "b = 5.0\n",
    "m = 1000\n",
    "prob=0.01\n",
    "\n",
    "NoisySGD_laplace = Composed_SubsampledLaplace_mech(prob=gamma, b=b,niter=m,name=\"NoisySGD_Laplace\")\n",
    "\n",
    "PrivateKNN_laplace = Composed_SubsampledLaplace_mech(prob=gamma, b=b,niter=m,name=\"PrivateKNN_Laplace\")\n",
    "        \n",
    "print(PrivateKNN_laplace.get_approxDP(delta=1e-6))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
